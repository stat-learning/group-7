---
title: "College characteristics and post-graduate earnings"
author: 
- "Alice Chang"
- "Ben Thomas"
- "Olek Wojcik"
date: "12/11/2019"
output: pdf_document
---

# Abstract

In this project, we aim to determine which factors have the strongest influence on the earnings of graduates from different institutions of higher education. To do so, we use data from the College Scorecard to predict the earnings of the median graduate from a specific institution 6 years after graduation. On top of personal characteristics such as demographics and family income, we find that institutional characteristics such as ___ may lead to higher incomes post-graduation. To estimate earnings, we used a number of different statistical techniques: (1) an OLS regression, with predictors pulled from the literature; (2) a random forest, with variable importances. Note also that due to the sheer amount of missing data in our dataset, we estimated the relationship between our predictors and earnings both without imputation, which removed many observations, as well as with observations imputed with a random forest technique.  

# Introduction
In the past decade, concern with analyzing and publicizing the employment outcomes of higher educational institutions and career training programs has become increasingly widespread. The release of the College Scorecard dataset by the Obama Administration in 2013 facilitated greater attention to the capacity of incoming undergraduate students to negotiate both the economic cost and prospective pay-off of higher education. In a public announcement, the U.S. Department of Education expressed their concern with enabling students tp make informed and economically sound college decisions, citing the Obama Administration’s commitment to providing “hardworking” students  “a real opportunity to earn an affordable, high quality degree or credential that offers a clear path to economic security and success.” 

## Theories of Inequality
Scholars examining the politics of social inequality have 

Equality of opportunity v. condition



# Data

## Download and clean

To approach this problem, we use data from the College Scorecard, a project of the United States Department of Education. Below, we start by downloading the data. Note that we're starting here with data only from the 2014-15 academic year. 

```{r Data download, message=FALSE, warning=FALSE}
library(tidyverse)
library(Amelia)
college.data <- read.csv("2014_2015_college_data.csv", header = TRUE)
```

This dataset contains 7,703 observations of 1,977 variables. Each observation represents an individual college/university for the 2014-15 school year. 

Now that we have this data, we need to transform it so that R can recognize missing values. Additionally, we (1) drop observations which do not contain our response variable, `MD_EARN_WNE_P6`, which is the "median earnings of students working and not enrolled 6 years after entry," and (2) drop variables for which all observations are N/A.  

```{r Clean the data}
library(dplyr)
library(tidyverse)
not_all_na <- function(x) any(!is.na(x))
college.data <- college.data %>%
  replace(.=="NULL", NA) %>%
  replace(.== "PrivacySuppressed", NA)

# Drop NAs in response
college.data.section <- college.data %>% 
  drop_na(MD_EARN_WNE_P6) %>%
  select_if(not_all_na)
```

There are a lot of variables to work with in this dataset--too many, in fact. To get around this, we went through our dataset and narrowed down the variables which we have reason to believe could be related to earnings after graduation. These variables range from characteristics of the college, such as the region it is located in or the amount it spends on each student, to characteristics of the student body, such as the share of the graduates who had ever received a Pell Grant. 

Note also that we have limited the scope of our analysis to only four-year colleges (`ICLEVEL == 1`), as it's reasonable to think that the factors that are important for graduates of two-year or more specialized institutions would be different. 

```{r Filter variables}

section1 <- college.data.section %>%
  filter(ICLEVEL == 1) %>%
  select(INSTNM,
         REGION, 
         CONTROL, 
         NUMBRANCH, 
         HIGHDEG, 
         PCIP01:PCIP54, 
         UGDS:PPTUG_EF, 
         PCTPELL, 
         INEXPFTE, 
         TUITFTE, 
         PCTFLOAN, 
         WDRAW_ORIG_YR4_RT, 
         FIRSTGEN_WDRAW_ORIG_YR4_RT, 
         LO_INC_WDRAW_ORIG_YR4_RT, 
         DEBT_MDN, 
         LO_INC_DEBT_MDN:NOTFIRSTGEN_DEBT_MDN, 
         FEMALE, 
         LOAN_EVER:FAMINC, -VETERAN, 
         FAMINC,
         MD_EARN_WNE_P6)
```

In this dataframe, unfortunately not all of the variables are classified appropriately (as a float, a factor, etc.). In order to get this data in workable format, we split up the data into the numeric and factor variables, classify each appropriately, and join them together again. 

```{r Numeric classification}
# Select numeric columns
numerics <- section1 %>%
  select(NUMBRANCH:MD_EARN_WNE_P6) %>%
  select(-HIGHDEG)

# Make columns numeric
numerics[,] <- sapply(numerics[,], as.numeric)
```

```{r Categorical classification}
# Select categorical columns
categoricals <- section1 %>%
  select(REGION, 
         CONTROL,
         HIGHDEG)

# Make columns factors
categoricals[,] <- sapply(categoricals[,], as.factor)
```

```{r Create final dataset}
# Create final dataframe
df_final <- cbind(numerics, 
                  categoricals,
                  INSTNM = section1$INSTNM)
```

## Exploratory data analysis

With the data now in workable format, we can start to take a look at our response and predictors.

PASTE IN EDA HERE, USE NEW DATA THOUGH

```{r}
# Correlation matrix
library(caret)
corr_matrix <- cor(numerics, use = "complete.obs")
```

```{r}
# Remove highly correlated variables
highly_correlated <- findCorrelation(corr_matrix, cutoff=0.75)
sort(highly_correlated)
numerics.noncorr <- numerics[-c(highly_correlated)]
```

```{r Visualize missingness}
missmap(df_final)
```

### Principal component analysis 

Conducting Principle Component Analysis can be useful in order to get a sense of what our data looks like and what different kinds of colleges we're dealing with. There could be different "clusters" of colleges-- ones that focus more on certain kinds of areas of study, or are more or less career focuses, etc. 

```{r PCA graph}
set.seed(40)
pca1 <- prcomp(drop_na(numerics))
d <- data.frame(PC1 = pca1$x[, 1],
                PC2 = pca1$x[, 2])
ggplot(d, aes(x = PC1, y = PC2)) +
  geom_point(alpha = 0.5)
```

There seems to be a bit of clustering, but overall the cases seem to be spread out in a relatively even way. 

```{r Scree Plot}
screeplot(pca1)
```

The scree plot demonstrates that most of the variration is captured in the first principle component. This means that there are not many different kinds of schools, and that there is a pretty uniform relationship among the variables that predict median wage.

## Imputation

As you might note in our exploratory data analysis, there's quite a bit of missing data in our dataset. One way to get around this issue is to attempt to impute the missing observations using the other predictors.

There are two different methods of imputing missing data that we use: (1) multiple imputation using the `mice` package, which uses predictive mean matching and logistic regression to perform multiple imputation on continuous and categorical data; and (2) multiple imputation using `missForest`, which uses a random forest model to predict each missing observation, no matter the type thereof. Note that both of these techniques assume that the missing data is "missing at random" (MAR). This is likely not the case with our data, so results generated from the imputed data should be taken with a grain of salt. 

### Multiple imputation with `mice`

In order to impute our missing values using the `mice` package, we need to impute the continuous variables and categorical variables separately. To impute the continuous variables, we will use predictive mean matching. Fortunately for us, there are no missing observations for the categorical variables, so we do not have to impute anything there.

```{r Install and load mice package, message=FALSE, warning=FALSE}
if(!require(mice)){
    install.packages("mice")
}
library(mice)
```

Below we impute the continuous variables. 

```{r Impute continuous variables, message=FALSE, cache=TRUE}
numerics.mice <- mice(numerics, m=5, maxit = 50, method = 'pmm', seed = 500)

numerics.mice.df1 <- complete(numerics.mice, 1)
numerics.mice.df2 <- complete(numerics.mice, 2)
numerics.mice.df3 <- complete(numerics.mice, 3)
numerics.mice.df4 <- complete(numerics.mice, 4)
numerics.mice.df5 <- complete(numerics.mice, 5)
```

```{r Combine numerical imputed values into one dataset}
# Create empty dataframe
df_numerics.mice <- data.frame(matrix(NA, nrow = 2822, ncol = 78))

# Replace cells with the mean of the 5 imputed dataframes
for (i in 1:2822) {
  for (j in 1:78) {
    df_numerics.mice[i,j] = mean(numerics.mice.df1[i,j],
                                 numerics.mice.df2[i, j],
                                 numerics.mice.df3[i,j],
                                 numerics.mice.df4[i,j],
                                 numerics.mice.df5[i,j])
  }
}

# Rename columns to the correct names
for (i in 1:78){
  names(df_numerics.mice)[i] = names(numerics)[i]
}
```

```{r Create final mice imputed dataset}
df.final.mice <- cbind.data.frame(df_numerics.mice,
                                  categoricals)
```

### Multiple imputation with `missForest`

Below, we use the `missForest` package to impute our missing data. Unlike the `mice` package, we can do this imputation in one step, which makes it much more simple. 

```{r Install and load missForest package}
if(!require(missForest)){
    install.packages("missForest")
}
library(missForest)
```

```{r Impute variables with missForest, cache=TRUE}
missForest.imputation <- missForest(df_final)
```

```{r, cache=TRUE}
missForest_df <- missForest.imputation$ximp
```

## Modeling

With our data now in workable format, with two different ways of imputing missingness, we now turn to modeling our data. 

### Linear models

Following the literature cited in our introduction, we built a theoretical model on what might impact graduates' earnings. This model is described with the equation below.

$$median \space earnings = \alpha + \beta_1 college \space characteristics + \beta_2 demographics + \epsilon$$

To capture **college characteristics**, we use the following variables:
- percentage of degrees awarded in many different majors (`PCIP01`, `PCIP02`, etc.)
- size of the school (`UGDS`)
- instructional expenditures per student (`INEXPFTE`)
- region of the school (`REGION`)
- share of students that are part-time (`PPTUG_EF`)
- tuition revenue per student (`TUITFTE`)
- ownership of school (`CONTROL`)
- highest degree awarded (`HIGHDEG`)

To capture **student demographics**, we use the following variables from our dataset:
- racial makeup of the school (`UGDS_WHITE`)
- debt after graduation (`DEBT_MDN`)
- gender makeup of school (`FEMALE`)
- age makeup of school (`AGE_ENTRY`)
- share of students that are first-generation (`FIRST_GEN`)
- median family income (`FAMINC`)
- share of students who have received a loan (`LOAN_EVER`)
- share of students who have received a Pell Grant (`PELL_EVER`)

#### Without imputation

Since we essentially have three datasets with these variables, it's an interesting exercise to see how the results of our analysis might differ with and without imputation. To start, we'll run a linear model of the above specification on the dataset without any imputation. 

```{r Linear model without imputation}
linear.model.1 <- lm(MD_EARN_WNE_P6 ~ 
                       PCIP01 +
                       PCIP03 +
                       PCIP04 +
                       PCIP05 +
                       PCIP09 +
                       PCIP10 +
                       PCIP11 +
                       PCIP12 +
                       PCIP13 + 
                       PCIP14 +
                       PCIP15 +
                       PCIP16 +
                       PCIP19 +
                       PCIP22 +
                       PCIP23 +
                       PCIP24 +
                       PCIP25 +
                       PCIP26 +
                       PCIP27 +
                       PCIP29 +
                       PCIP30 +
                       PCIP31 + 
                       PCIP38 +
                       PCIP39 +
                       PCIP40 +
                       PCIP41 +
                       PCIP42 +
                       PCIP43 +
                       PCIP44 +
                       PCIP45 +
                       PCIP46 +
                       PCIP47 +
                       PCIP48 +
                       PCIP49 +
                       PCIP50 +
                       PCIP51 +
                       PCIP52 +
                       PCIP54+
                       UGDS +
                       UGDS_WHITE +
                       INEXPFTE +
                       DEBT_MDN +
                       FEMALE +
                       AGE_ENTRY +
                       FIRST_GEN +
                       REGION +
                       PPTUG_EF +
                       TUITFTE + 
                       LOAN_EVER +
                       FAMINC +
                       CONTROL +
                       PELL_EVER +
                       HIGHDEG, 
                     data = df_final)
summary(linear.model.1)

#Run model diagnostics

#Residual plot
plot(linear.model.1, 1)

#QQ [;pt
plot(linear.model.1, 2)]

#Scale location plot
plot(linear.model.1, 3)

#Residual 2 
plot(linear.model.1, 4)

#Quartet
par(mfrow = c(2, 2))
plot(linear.model.1)

#Check for multicollinearity
library(olsrr)

library(caret)
ols_vif_tol(linear.model.1)
corr_matrix <- cor(linear.model.1, use = "complete.obs")
```

```{r}
kitchen.sink.imputed <- lm(MD_EARN_WNE_P6 ~ ., data = imputed_df)
summary(kitchen.sink.imputed)
```

#### Random forest modeling

In order to get a sense of what variables are deemed most important by a more data-oriented approach, we decided to run a random forest on our data set and measure variable importance.

```{r Random Forest}
df_forest <- df_final %>%
  select(-INSTNM)

library(randomForest)
set.seed(1)
rf.college = randomForest(MD_EARN_WNE_P6  ~., data = df_forest, na.action = na.roughfix, importance = TRUE)
importance(rf.college)
varImpPlot(rf.college, cex = 0.6)
```

```{r Print Random Forest}
print(rf.college)
```

### Discussion

### Sources


