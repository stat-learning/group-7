---
title: "College characteristics and post-graduate earnings"
author: 
- "Alice Chang"
- "Ben Thomas"
- "Olek Wojcik"
date: "12/11/2019"
output: pdf_document
---

```{r Load libraries, warning=FALSE, message=FALSE, include=FALSE}
library(tidyverse)
if(!require(Amelia)){
    install.packages("Amelia")
}
library(Amelia)
if(!require(corrplot)){
    install.packages("corrplot")
}
library(corrplot)
if(!require(caret)){
    install.packages("caret")
}
library(caret)
if(!require(mice)){
    install.packages("mice")
}
library(mice)
if(!require(missForest)){
    install.packages("missForest")
}
library(missForest)
if(!require(olsrr)){
    install.packages("olsrr")
}
library(olsrr)
if(!require(randomForest)){
    install.packages("randomForest")
}
library(randomForest)
if(!require(ggrepel)){
    install.packages("ggrepel")
}
library(ggrepel)
```

# Abstract

In this project, we aim to determine which factors have the strongest influence on the earnings of graduates from different institutions of higher education. To do so, we use data from the College Scorecard to predict the earnings of the median graduate from a specific institution 6 years after graduation. On top of personal characteristics such as demographics and family income, we find that institutional characteristics such as ___ may lead to higher incomes post-graduation. To estimate earnings, we used a number of different statistical techniques: (1) an OLS regression, with predictors pulled from the literature; (2) a random forest, with variable importances. Note also that due to the sheer amount of missing data in our dataset, we estimated the relationship between our predictors and earnings both without imputation, which removed many observations, as well as with observations imputed with a random forest technique.  

# Introduction

In the past decade, concern with analyzing and publicizing the employment outcomes of higher educational institutions and career training programs has become increasingly widespread. The release of the College Scorecard dataset by the Obama Administration in 2013 facilitated greater attention to the capacity of incoming undergraduate students to negotiate both the economic cost and prospective pay-off of higher education. In a public announcement, the U.S. Department of Education expressed their concern with enabling students tp make informed and economically sound college decisions, citing the Obama Administration’s commitment to providing “hardworking” students  “a real opportunity to earn an affordable, high quality degree or credential that offers a clear path to economic security and success.” 

## Theories of Inequality

Scholars examining the politics of social inequality have 

Equality of opportunity v. condition

# Data

## Download and clean

To approach this problem, we use data from the College Scorecard, a project of the United States Department of Education. Below, we start by downloading the data. Note that we're starting here with data only from the 2014-15 academic year. 

```{r Data download, message=FALSE, warning=FALSE, cache=TRUE}
college.data <- read.csv("2014_2015_college_data.csv", header = TRUE)
```

This dataset contains 7,703 observations of 1,977 variables. Each observation represents an individual college/university for the 2014-15 school year. 

Now that we have this data, we need to transform it so that R can recognize missing values. Additionally, we (1) drop observations which do not contain our response variable, `MD_EARN_WNE_P6`, which is the "median earnings of students working and not enrolled 6 years after entry," and (2) drop variables for which all observations are N/A.  

```{r Clean the data}
not_all_na <- function(x) any(!is.na(x))
college.data <- college.data %>%
  replace(.=="NULL", NA) %>%
  replace(.== "PrivacySuppressed", NA)

# Drop NAs in response
college.data.section <- college.data %>% 
  drop_na(MD_EARN_WNE_P6) %>%
  select_if(not_all_na)
```

There are a lot of variables to work with in this dataset--too many, in fact. To get around this, we went through our dataset and narrowed down the variables which we have reason to believe could be related to earnings after graduation. These variables range from characteristics of the college, such as the region it is located in or the amount it spends on each student, to characteristics of the student body, such as the share of the graduates who had ever received a Pell Grant. 

Note also that we have limited the scope of our analysis to only four-year colleges (`ICLEVEL == 1`), as it's reasonable to think that the factors that are important for graduates of two-year or more specialized institutions would be different. 

```{r Filter variables}
section1 <- college.data.section %>%
  filter(ICLEVEL == 1) %>%
  select(INSTNM,
         REGION, 
         CONTROL, 
         NUMBRANCH, 
         HIGHDEG, 
         PCIP01:PCIP54, 
         UGDS:PPTUG_EF, 
         PCTPELL, 
         INEXPFTE, 
         TUITFTE, 
         PCTFLOAN, 
         WDRAW_ORIG_YR4_RT, 
         DEBT_MDN, 
         LO_INC_DEBT_MDN:HI_INC_DEBT_MDN,
         PELL_DEBT_MDN:NOTFIRSTGEN_DEBT_MDN, 
         FEMALE, 
         LOAN_EVER:AGE_ENTRY,
         DEPENDENT:FAMINC, -VETERAN, 
         FAMINC,
         MD_EARN_WNE_P6)
```

In this dataframe, unfortunately not all of the variables are classified appropriately (as a float, a factor, etc.). In order to get this data in workable format, we split up the data into the numeric and factor variables, classify each appropriately, and join them together again. 

```{r Numeric classification}
# Select numeric columns
numerics <- section1 %>%
  select(NUMBRANCH:MD_EARN_WNE_P6) %>%
  select(-HIGHDEG)

# Make columns numeric
numerics[,] <- sapply(numerics[,], as.numeric)
```

```{r Categorical classification}
# Select categorical columns
categoricals <- section1 %>%
  select(REGION, 
         CONTROL,
         HIGHDEG)

categoricals <- categoricals %>%
  mutate(REGION = factor(REGION),
         CONTROL = factor(CONTROL),
         HIGHDEG = factor(HIGHDEG))
```

```{r Create final dataset}
# Create final dataframe
df_final <- cbind(INSTNM = as.character(section1$INSTNM),
                  numerics, 
                  categoricals)

df_final <- df_final %>%
  mutate(INSTNM = as.character(INSTNM))
```

## Exploratory data analysis

With the data now in workable format, we can start to take a look at our response and predictors. Our new dataset has 2,822 observations of 77 variables--significantly cut down from our initial one. 

### Response variable

A histogram of our response variable (`MD_EARN_WNE_P6`) is shown below. Note that while this variable is distributed fairly normally, it has a long right tail.  

```{r Histogram of response}
ggplot(data = df_final, aes(df_final$MD_EARN_WNE_P6)) + 
  geom_histogram(binwidth = 10) +
  labs(x = "Median wage", y = "Frequency") +
  theme_minimal()
```

One way to reduce the skew in our dependent would be to take the natural log of it. (This is common practice in Economics for income variables.) When we make a new variable equal to the log of the median wage, its histogram is much more "normal-looking." Moving forward, we use this new response variable as our dependent variable.

```{r Histogram of log median wage}
df_final <- df_final %>%
  mutate(log_md_wage = log(MD_EARN_WNE_P6) + 0.001)

ggplot(data = df_final, aes(df_final$log_md_wage)) + 
  geom_histogram(bins = 40) +
  labs(x = "Log of the median wage", y = "Frequency") +
  theme_minimal()
```

### Pairwise scatterplots

Before we jump in to any modeling, we should understand the relationship our dependent variable has with predictors of interest. Some interesting pairwise scatterplots are displayed below. 

```{r Logwage v faminc, message=FALSE, warning=FALSE}
wage.faminc.plot <- ggplot(df_final, aes(x=FAMINC, y=log_md_wage)) + 
  geom_point(alpha = 0.5) +
  labs(x = "Family income", y = "Log of the median wage") +
  theme_minimal()
wage.faminc.plot
```

The relationship between family income and the log of the median wage seems fairly linear here, necessitating no transformation. 

```{r Logwage v pell, message=FALSE, warning=FALSE}
wage.pell.plot <- ggplot(df_final, aes(x=PELL_EVER, y=log_md_wage)) + 
  geom_point(alpha = 0.5) +
  labs(x = "Number of students who have received a Pell Grant", y = "Log of the median wage") +
  theme_minimal()
wage.pell.plot
```

This, too, seems to be a linear relationship, requiring no transformation. 

One relationship that does require a transformation is that below, of log wage versus the median wage after graduation. Note that there is a seeming discontinuity between this relationship after the median debt exceeds 1000. 

```{r Logwage v median debt, warning = FALSE, message = FALSE}
wage.meddebt.plot <- ggplot(df_final, aes(x=DEBT_MDN, y=log_md_wage)) + 
  geom_point(alpha = 0.2) +
  labs(x = "Median debt", y = "Log of the median wage") +
  theme_minimal()
wage.meddebt.plot
```

```{r Logwage v log median debt, warning = FALSE, message = FALSE}
df_final <- df_final %>%
  mutate(logdebt = log(DEBT_MDN))

wage.logmeddebt.plot <- ggplot(df_final, aes(x=logdebt, y=log_md_wage)) + 
  geom_point(alpha = 0.2) +
  labs(x = "Log of median debt", y = "Log of the median wage") +
  theme_minimal()
wage.logmeddebt.plot
```

While still not perfect, taking the log of the median debt seems to make this relationship a little better. 

### Correlations

We can dive into this data a little more deeply using a correlation matrix. We've made one and plotted it below. 

```{r Correlation matrix}
# Add in new variables into numerics
numerics <- numerics %>%
  mutate(logdebt = log(DEBT_MDN),
         log_md_wage = log(MD_EARN_WNE_P6) + 0.001)

# Correlation matrix
corr_matrix <- cor(numerics, use = "complete.obs")
corrplot(corr_matrix, method="color", tl.cex = 0.4)
```

Much of the space in this matrix is taken up by the `PCIP01`, `PCIP03`, etc. variables. To zoom in on the other variables, we can take these out and re-run our correlation matrix.

```{r Correlation matrix 2, cache=TRUE}
corr_matrix2 <- cor(numerics[,40:ncol(numerics)], use = "complete.obs")
corrplot(corr_matrix2, method="color", tl.cex = 0.5)
```

As we'd expect, many of the debt measures are highly correlated with each other. Additionally, there are very strongly positive and strongly negative correlations in a cluster toward the bottom right of this graph, between variables like `FAMINC`, `FIRST_GEN`, and `AGE_ENTRY`. 

### Missingness

One piece of the pie that proved tricky for us moving forward in our analysis has been the amount of missingness present in the College Scorecard dataset. As shown below, fully 61% of the data in the 2014-2015 College Scorecard file is missing. (Note that we've actually attached a picture instead of the code output--the image generated by the code itself takes around 40 Mb.)

```{r Visualize missingness for full dataset, eval = FALSE}
missmap(college.data)
```

![](Missingness.png)

Fortunately for us, after all of our data cleaning, this picture looks much better. In our final dataset (`df_final`), 93% of observations are present. 

One interesting thing to note in this missingness map is the giant block of missing data in the bottom left. This missing data chunk, clearly not missing at random, is comprised mostly of schools such as Keiser University, Harrison College, Rasmussen College, Strayer University. These schools have a couple things in common: for one, all have multiple branches, and perhaps most tellingly, these are not categorized as degree-granting institutions. For these schools, the College Scorecard does not report the shares of degrees awarded in a particular major, the racial makeup of these schools, or how much tuition revenue these schools receive per student.

```{r Visualize missingness for df_final, cache=TRUE}
missmap(df_final)
```

In our scenario, observations are not exactly a precious resource; we could afford to get rid of this chunk of observations. This has some benefits as well. In addition to being theoretically sound (as non-degree granting institutions may play a different role in determining future income than degree-granting institutions), these observations are distinctly not missing at random. This means that any imputation (done later) to determine these values would be fundamentally flawed. 

After ridding the dataset of this chunk of observations, our new missingnessmap looks as follows. Note that fully 98% of our observations are present. Not too bad! 

```{r Visualize missingness without red chunk, cache=TRUE}
df_final_section <- df_final[1:2634,]
missmap(df_final_section)
```

### Principal component analysis 

One last method to examine the structure and content of our data is principal component analysis (PCA). Conducting principle component analysis can be quite useful in order to get a sense of what our data looks like and what different kinds of colleges we're dealing with. After finding the primary axes of variation (principal components), we are able to determine whether there are different "clusters" of colleges: ones that focus more on certain kinds of areas of study, are more or less career focused, etc. 

```{r PCA generation and plot}
# Drop odd chunk of obs from numerics dataset
numerics.section <- numerics[1:2634,]
numerics.nona <- drop_na(numerics.section)
df_final_section_nona <- drop_na(df_final_section)

# PCA
set.seed(40)
pca <- prcomp(numerics.nona, scale. = TRUE)
pcs <- as.data.frame(pca$x)
pc1 <- pca$rotation[, 1]
pc2 <- pca$rotation[, 2]
pca3 <- pca$rotation[, 3]
pcs$INSTNM <- df_final_section_nona$INSTNM

# Plot
ggplot(pcs, aes(x = PC1, y = PC2)) +
  geom_point(alpha = .5) +
  theme_minimal()
```

This is a very interesting biplot. Along the first two principal components, there are two clear groups, one of which has a greater PC2 value than the other. Additionally, in the group of observations at the bottom half of the biplot, there appears to be the most density at high values for PC1 and low values for PC2. In the upper group of observations, the most density occurs at low/mid values of PC1 and moderate values of PC2. 

```{r Scree plot}
scree.data <- tibble(PC = 1:75,
                     PVE = pca$sdev^2 /
                       sum(pca$sdev^2))
ggplot(scree.data, aes(x = PC, y = PVE)) +
  geom_line() + 
  geom_point() +
  theme_minimal()
```

As seen in the scree plot above, there is a clear, distinct elbow in our data. The first two principal components explain a little less 30% of the variation in the data, after which each individual principal component becomes increasingly meaningless. 

#### A deeper dive

Clearly, the first and second principal components capture important sources of variation among colleges. However, we don't yet know what they actually are. Taking inspiration from group 9 (Paul Nguyen and David Herrero Quevedo), we dove deeper into which variables are actually important in making up these components. 

As an initial step, it may be illuminating to label a random sample of these observations to see if any obvious patterns pop out. After, we plot the location of traditional groups (e.g. the Ivies) to see if there is any validity to grouping these elite colleges together. 

```{r Sample schools in biplot}
set.seed(532)
random.schools <- sample(pcs$INSTNM, 10)
ivies <- c("Brown University", "Columbia University", "Cornell University", "Dartmouth College", "Harvard University", "University of Pennsylvania", "Princeton University", "Yale University")

# Plot random schools
ggplot(pcs, aes(x = PC1, y = PC2)) +
  geom_point(alpha = .1) +
  geom_text_repel(data = subset(pcs, INSTNM %in% random.schools),
            aes(label = INSTNM)) +
  geom_point(color = "red",
             data = subset(pcs, INSTNM %in% random.schools,
             alpha = .5)) +
  theme_minimal()
```

Unfortunately, it's not immediately obvious through labeling some schools on these principal components what the components actually mean. To try to get more recognizable names, we've labeled the Ivy League.

```{r Ivies in biplot}
ggplot(pcs, aes(x = PC1, y = PC2)) +
  geom_point(alpha = .1) +
  geom_text_repel(data = subset(pcs, INSTNM %in% ivies),
            aes(label = INSTNM)) +
  geom_point(color = "red",
             data = subset(pcs, INSTNM %in% ivies,
             alpha = .5)) +
  theme_minimal()
```

Clearly here, the Ivies (or at least the ones which survived our purge of non-NA data) are all on the left side of the biplot, indicating that the left side of the plot (when PC1 is negative) may be a region with more prestigious, smaller schools. Harvard is a notable outlier from both main groups. 

For sheer curiosity's sake, Reed's location on this biplot is shown below:

```{r Reed in biplot}
ggplot(pcs, aes(x = PC1, y = PC2)) +
  geom_point(alpha = .1) +
  geom_text_repel(data = subset(pcs, INSTNM %in% c("Reed College")),
            aes(label = INSTNM)) +
  geom_point(color = "red",
             data = subset(pcs, INSTNM %in% c("Reed College"),
             alpha = .5)) +
  theme_minimal()
```

Good news for Reed graduates; we're almost in exactly the same spot as most of the Ivies! 

While plotting the locations of any given college on the principal component biplot is fun (and a little informative), it doesn't give us a deep understanding of what these components mean. Moving forward, we took a more statistical approach toward understanding this. 

Below, we've plotted the "rotation" of our first principal component. Essentially, this plot shows how each variable is weighted when making PC1: a strong, positive number indicates that PC1 is strongly associated with a given variable, and vice versa.  

```{r Understanding PC1}
pca_rotations <- data.frame(pca$rotation)
pca_rotations$variables <- rownames(pca_rotations)
pc1.ordered.rotations <- pca_rotations[order(-pca_rotations$PC1),]
pc2.ordered.rotations <- pca_rotations[order(-pca_rotations$PC2),]

ggplot(pca_rotations, 
       aes(x = variables, y = PC1)) +
  geom_col() +
  coord_flip() +
  labs(x = "Variables", y = "First PC") +
  theme(text = element_text(size = 5.5)) 
```

Due to the sheer amount of variables in our PCA, it's hard to read which variables are positively and negatively correlated with the first PC. For the reader's sake, we've also sorted the variables by their impact on PC1. 

As seen below, the 6 variables most associated with PC1 are: 

1. `FIRST_GEN`
2. `AGE_ENTRY`
3. `PELL_EVER`
4. `PCTPELL`
5. `WDRAW_ORIG_YR4_RT`
6. `LO_INC_DEBT_MDN`

The six most negatively associated with PC1 are: 

1. `DEPENDENT`
2. `PCIP26`
3. `PCIP45`
4. `PCIP23`
5. `PCIP54`
6. `FAMINC`

One way of interpreting PC1, then, could be as a measure of the privelege (or lack thereof) of the students. 

```{r PC1 associated variables}
head(pc1.ordered.rotations)
tail(pc1.ordered.rotations)
```

We can do the same thing with the second principal component. 

As seen below, the 6 variables most correlated with PC2 are:

1. `PCIP15`
2. `NUMBRANCH`
3. `PCIP11`
4. `PCTFLOAN`
5. `PCTPELL`
6. `PCIP10`

The six most negatively correlated with PC2 are:

1. `logdebt`
2. `FEMALE_DEBT_MDN`
3. `FIRSTGEN_DEBT_MDN`
4. `PELL_DEBT_MDN`
5. `NOTFIRSTGEN_DEBT_MDN`
6. `DEBT_MDN`

This principal component is a little less intuitive to understand, but clearly relates somewhat to the indebtedness of the student body, in addition to their specific major interests. Higher PC2 values may imply more of a focus on more rewarded majors (such as engineering or computer science).

```{r PC2 associated variables}
head(pc2.ordered.rotations)
tail(pc2.ordered.rotations)
```

## Imputation

As you might note in our exploratory data analysis, there's quite a bit of missing data in our dataset. One way to get around this issue is to attempt to impute the missing observations using the other predictors.

We use two different methods to impute missing data: (1) multiple imputation using the `mice` package, which uses predictive mean matching and logistic regression to perform multiple imputation on continuous and categorical data; and (2) imputation using `missForest`, which uses a random forest model to predict each missing observation, no matter the type thereof. Note that both of these techniques assume that the missing data is "missing at random" (MAR). While this is more likely the case after our data cleaning, this is still likely a stretch, so any results generated from the imputed data should be taken with a grain of salt. 

### Imputation with `mice`

In order to impute our missing values using the `mice` package, we need to impute the continuous variables and categorical variables separately. To impute the continuous variables, we will use predictive mean matching. Fortunately for us, there are no missing observations for the categorical variables, so we do not have to impute anything there.

Below we impute the continuous variables. 

```{r Impute continuous variables, message=FALSE, cache=TRUE}
numerics.mice <- mice(numerics, m=5, maxit = 50, method = 'pmm', seed = 500)

numerics.mice.df1 <- complete(numerics.mice, 1)
numerics.mice.df2 <- complete(numerics.mice, 2)
numerics.mice.df3 <- complete(numerics.mice, 3)
numerics.mice.df4 <- complete(numerics.mice, 4)
numerics.mice.df5 <- complete(numerics.mice, 5)
```

```{r Combine numerical imputed values into one dataset}
# Create empty dataframe
df_numerics.mice <- data.frame(matrix(NA, nrow = nrow(numerics.mice.df1), ncol = ncol(numerics.mice.df1)))

# Replace cells with the mean of the 5 imputed dataframes
for (i in 1:nrow(numerics.mice.df1)) {
  for (j in 1:ncol(numerics.mice.df1)) {
    df_numerics.mice[i,j] = mean(numerics.mice.df1[i,j],
                                 numerics.mice.df2[i,j],
                                 numerics.mice.df3[i,j],
                                 numerics.mice.df4[i,j],
                                 numerics.mice.df5[i,j])
  }
}

# Rename columns to the correct names
for (i in 1:ncol(df_numerics.mice)){
  names(df_numerics.mice)[i] = names(numerics)[i]
}
```

```{r Create final mice imputed dataset}
df.final.mice <- cbind.data.frame(INSTNM = df_final$INSTNM,
                                  df_numerics.mice,
                                  categoricals)

df.final.mice <- df.final.mice %>%
  mutate(INSTNM = as.character(INSTNM))
```

### Multiple imputation with `missForest`

Below, we use the `missForest` package to impute our missing data. Unlike the `mice` package, we can do this imputation in one step, which makes it much more simple. 

```{r Impute variables with missForest, cache=TRUE}
missForest.imputation <- missForest(df_final)
```

```{r, cache=TRUE}
missForest_df <- missForest.imputation$ximp
```

## Modeling

With our data now in workable format, with two different ways of imputing missingness, we now turn to modeling our data. 

### Linear models

Following the literature cited in our introduction, we built a theoretical model on what might impact graduates' earnings. This model is described with the equation below.

$$median \space earnings = \alpha + \beta_1 college \space characteristics + \beta_2 student \space demographics + \epsilon$$

To capture **college characteristics**, we use the following variables:
- percentage of degrees awarded in many different majors (`PCIP01`, `PCIP02`, etc.)
- size of the school (`UGDS`)
- instructional expenditures per student (`INEXPFTE`)
- region of the school (`REGION`)
- share of students that are part-time (`PPTUG_EF`)
- tuition revenue per student (`TUITFTE`)
- ownership of school (`CONTROL`)
- highest degree awarded (`HIGHDEG`)

To capture **student demographics**, we use the following variables from our dataset:
- racial makeup of the school (`UGDS_WHITE`)
- debt after graduation (`DEBT_MDN`)
- gender makeup of school (`FEMALE`)
- age makeup of school (`AGE_ENTRY`)
- share of students that are first-generation (`FIRST_GEN`)
- median family income (`FAMINC`)
- share of students who have received a loan (`LOAN_EVER`)
- share of students who have received a Pell Grant (`PELL_EVER`)

#### Without imputation

Since we essentially have three datasets with these variables, it's an interesting exercise to see how the results of our analysis might differ with and without imputation. To start, we'll run a linear model of the above specification on the dataset without any imputation. 

```{r Linear model without imputation}
linear.model.1 <- lm(log_md_wage ~ 
                       PCIP01 +
                       PCIP03 +
                       PCIP04 +
                       PCIP05 +
                       PCIP09 +
                       PCIP10 +
                       PCIP11 +
                       PCIP12 +
                       PCIP13 + 
                       PCIP14 +
                       PCIP15 +
                       PCIP16 +
                       PCIP19 +
                       PCIP22 +
                       PCIP23 +
                       PCIP24 +
                       PCIP25 +
                       PCIP26 +
                       PCIP27 +
                       PCIP29 +
                       PCIP30 +
                       PCIP31 + 
                       PCIP38 +
                       PCIP39 +
                       PCIP40 +
                       PCIP41 +
                       PCIP42 +
                       PCIP43 +
                       PCIP44 +
                       PCIP45 +
                       PCIP46 +
                       PCIP47 +
                       PCIP48 +
                       PCIP49 +
                       PCIP50 +
                       PCIP51 +
                       PCIP52 +
                       PCIP54+
                       UGDS +
                       UGDS_WHITE +
                       INEXPFTE +
                       DEBT_MDN +
                       FEMALE +
                       AGE_ENTRY +
                       FIRST_GEN +
                       REGION +
                       PPTUG_EF +
                       TUITFTE + 
                       LOAN_EVER +
                       FAMINC +
                       CONTROL +
                       PELL_EVER +
                       HIGHDEG, 
                     data = df_final)
summary(linear.model.1)

#Run model diagnostics

#Residual plot
plot(linear.model.1, 1)

#QQ [;pt
plot(linear.model.1, 2)

#Scale location plot
plot(linear.model.1, 3)

#Residual 2 
plot(linear.model.1, 4)

#Quartet
par(mfrow = c(2, 2))
plot(linear.model.1)

#Check for multicollinearity
ols_vif_tol(linear.model.1)
corr_matrix <- cor(linear.model.1, use = "complete.obs")
```

```{r Linear model with imputation}
kitchen.sink.imputed <- lm(MD_EARN_WNE_P6 ~ ., data = imputed_df)
summary(kitchen.sink.imputed)
```

### Lasso
Since 
```{r Lasso model without imputation}
X <- model.matrix(MD_EARN_WNE_P6 ~ ., data = df_final)[, -77]
Y <- df_final$MD_EARN_WNE_P6 
grid <- seq(from = 1e4, to = 1e-2, length.out = 100)

lasso.1 <- glmnet(x = X, y = Y, alpha = 1,
lambda = grid, standardize = TRUE)

set.seed(1)
cv.out1 = cv.glmnet(X, Y, alpha = 1)
bestlam1 = cv.out1$lambda.min
bestlam1

#Rebuild with best lambda
lasso_best1 <- glmnet(x = X, y = Y, alpha = 1, lambda = best_lam1)

#Get coefficients of most important variables
coef(lasso_best1)
#Get training MSE for LASSO
lasso.pred = predict(lasso.best1, s = bestlam1, X)
MSE_lasso1 = mean ((lasso.pred - Y)^2)
MSE_lasso1

```


```{r Lasso model with imputation}
X.imp <- model.matrix(MD_EARN_WNE_P6 ~ ., data = imputed_df)[, -INSERTINDEXFORY!]
Y.imp <- imputed_df$MD_EARN_WNE_P6 
grid <- seq(from = 1e4, to = 1e-2, length.out = 100)

lasso.impute <- glmnet(x = X.imp, y = Y.imp, alpha = 1,
lambda = grid, standardize = TRUE)

set.seed(1)
cv.out.imp = cv.glmnet(X.imp, Y.imp, alpha = 1)
bestlam.imp = cv.out.imp$lambda.min
bestlam.imp

#Rebuild with best lambda
lasso.best.imp <- glmnet(x = X.imp, y = Y.imp, alpha = 1, lambda = bestlam.imp)

#Get coefficients of most important variables
coef(lasso.best.imp)
#Get training MSE for LASSO
lasso.pred.imp = predict(lasso.best.imp, s = bestlam.imp, X.imp)
MSE.lasso.imp = mean((lasso.pred.imp - Y.imp)^2)
MSE.lasso.imp

```

### Random forest modeling

In order to get a sense of what variables are deemed most important by a more data-oriented approach, we decided to run a random forest on our data set and measure variable importance.

```{r Random Forest}
df_forest <- df_final %>%
  select(-INSTNM)

set.seed(1)
rf.college = randomForest(log_md_wage  ~., data = df_forest, na.action = na.roughfix, importance = TRUE)
importance(rf.college)
varImpPlot(rf.college, cex = 0.6)
```

```{r Print Random Forest}
print(rf.college)
```

### Discussion

### Sources


