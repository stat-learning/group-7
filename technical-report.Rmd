---
title: "College characteristics and post-graduate earnings"
author: 
- "Alice Chang"
- "Ben Thomas"
- "Olek Wojcik"
date: "12/11/2019"
output: pdf_document
---

## Abstract

In this project, we aim to determine which factors have the strongest influence on the earnings of graduates from different institutions of higher education. To do so, we use data from the College Scorecard to predict the earnings of the median graduate from a specific institution 6 years after graduation. On top of personal characteristics such as demographics and family income, we find that institutional characteristics such as ___ may lead to higher incomes post-graduation. To estimate earnings, we used a number of different statistical techniques: (1) an OLS regression, with predictors pulled from the literature; (2) a random forest, with variable importances. Note also that due to the sheer amount of missing data in our dataset, we estimated the relationship between our predictors and earnings both without imputation, which removed many observations, as well as with observations imputed with a random forest technique.  

## Introduction

An enormous body of work has gone into understanding which factors lead to higher incomes. From a sociological lens, demographic factors such as race, gender, and family income are hugely important predictors of future earnings. From an economic perspective, ... CITE SOURCES.

## Data

### Download and clean

To approach this problem, we use data from the College Scorecard, a project of the United States Department of Education. Below, we start by downloading the data. Note that we're starting here with data only from the 2014-15 academic year. 

```{r Data download, message=FALSE, warning=FALSE}
library(tidyverse)
library(Amelia)
college.data <- read.csv("2014_2015_college_data.csv", header = TRUE)
```

This dataset contains 7,703 observations of 1,977 variables. Each observation represents an individual college/university for the 2014-15 school year. 

Now that we have this data, we need to transform it so that R can recognize missing values. Additionally, we (1) drop observations which do not contain our response variable, `MD_EARN_WNE_P6`, which is the "median earnings of students working and not enrolled 6 years after entry," and (2) drop variables for which all observations are N/A.  

```{r Clean the data}
not_all_na <- function(x) any(!is.na(x))
college.data <- college.data %>%
  replace(.=="NULL", NA) %>%
  replace(.== "PrivacySuppressed", NA)

# Drop NAs in response
college.data.section <- college.data %>% 
  drop_na(MD_EARN_WNE_P6) %>%
  select_if(not_all_na)
```

There are a lot of variables to work with in this dataset--too many, in fact. To get around this, we went through our dataset and narrowed down the variables which we have reason to believe could be related to earnings after graduation. These variables range from characteristics of the college, such as the region it is located in or the amount it spends on each student, to characteristics of the student body, such as the share of the graduates who had ever received a Pell Grant. 

Note also that we have limited the scope of our analysis to only four-year colleges (`ICLEVEL == 1`), as it's reasonable to think that the factors that are important for graduates of two-year or more specialized institutions would be different. 

```{r Filter variables}
section1 <- college.data.section %>%
  filter(ICLEVEL == 1) %>%
  select(REGION, 
         CONTROL, 
         NUMBRANCH, 
         HIGHDEG, 
         PCIP01:PCIP54, 
         UGDS:PPTUG_EF, 
         PCTPELL, 
         INEXPFTE, 
         TUITFTE, 
         PCTFLOAN, 
         WDRAW_ORIG_YR4_RT, 
         FIRSTGEN_WDRAW_ORIG_YR4_RT, 
         LO_INC_WDRAW_ORIG_YR4_RT, 
         DEBT_MDN, 
         LO_INC_DEBT_MDN:NOTFIRSTGEN_DEBT_MDN, 
         FEMALE, 
         LOAN_EVER:FAMINC, -VETERAN, 
         FAMINC,
         MD_EARN_WNE_P6)
```

In this dataframe, unfortunately not all of the variables are classified appropriately (as a float, a factor, etc.). In order to get this data in workable format, we split up the data into the numeric and factor variables, classify each appropriately, and join them together again. 

```{r Numeric classification}
# Select numeric columns
numerics <- section1 %>%
  select(NUMBRANCH:MD_EARN_WNE_P6) %>%
  select(-HIGHDEG)

# Make columns numeric
numerics[,] <- sapply(numerics[,], as.numeric)
```

```{r Categorical classification}
# Select categorical columns
categoricals <- section1 %>%
  select(REGION, 
         CONTROL,
         HIGHDEG)

# Make columns factors
categoricals[,] <- sapply(categoricals[,], as.factor)
```

```{r Create final dataset}
# Create final dataframe
df_final <- cbind(numerics, 
                  categoricals)
```

### Exploratory data analysis

With the data now in workable format, we can start to take a look at our response and predictors.

PASTE IN EDA HERE, USE NEW DATA THOUGH

```{r}
# Correlation matrix
library(caret)
corr_matrix <- cor(numerics, use = "complete.obs")
```

```{r}
# Remove highly correlated variables
highly_correlated <- findCorrelation(corr_matrix, cutoff=0.75)
sort(highly_correlated)
numerics.noncorr <- numerics[-c(highly_correlated)]
```

```{r Visualize missingness}
missmap(df_final)
```

#### Principal component analysis 

### Imputation

As you might note in our exploratory data analysis, there's quite a bit of missing data in our dataset. One way to get around this issue is to attempt to impute the missing observations using the other predictors.

There are two different methods of imputing missing data that we use: (1) multiple imputation using the `mice` package, which uses predictive mean matching and logistic regression to perform multiple imputation on continuous and categorical data; and (2) multiple imputation using `missForest`, which uses a random forest model to predict each missing observation, no matter the type thereof. Note that both of these techniques assume that the missing data is "missing at random" (MAR). This is likely not the case with our data, so results generated from the imputed data should be taken with a grain of salt. 

#### Multiple imputation with `mice`

In order to impute our missing values using the `mice` package, we need to impute the continuous variables and categorical variables separately. To impute the continuous variables, we will use predictive mean matching. For the categorical variables, we'll use logistic regression.

```{r Install and load mice package, message=FALSE, warning=FALSE}
if(!require(mice)){
    install.packages("mice")
}
library(mice)
```

Below we impute the continuous variables. 

```{r Impute continuous variables, message=FALSE, cache=TRUE}
numerics.mice <- mice(numerics, m=5, maxit = 50, method = 'pmm', seed = 500)
numerics.mice.df <- numerics.mice$imp
```

```{r Impute categorical variables}

```

```{r Combine into one dataset}

```

#### Multiple imputation with `missForest`

Below, we use the `missForest` package to impute our missing data. Unlike the `mice` package, we can do this imputation in one step, which makes it much more simple. 

```{r Install and load missForest package}

```

```{r Impute variables with missForest}
library(missForest)
missForest.imputation <- missForest(df_final)
```

```{r}
missForest_df <- missForest.imputation$ximp
```

### Modeling

#### Linear models

```{r}
kitchen.sink <- lm(MD_EARN_WNE_P6 ~ ., data = df_final)
summary(kitchen.sink)
```

```{r}
kitchen.sink.imputed <- lm(MD_EARN_WNE_P6 ~ ., data = imputed_df)
summary(kitchen.sink.imputed)
```

#### Random forest modeling

### Discussion

### Sources


